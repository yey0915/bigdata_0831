{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n",
      "[[ 3]\n",
      " [ 5]\n",
      " [ 7]\n",
      " [ 9]\n",
      " [11]]\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([1,2,3,4,5]).reshape(5,1)\n",
    "y_data = np.array([3,5,7,9,11]).reshape(5,1)  # y = 2x + 1\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data1 = np.array([[1,2], [2,3], [3,4], [4,5], [5,6]]).reshape(5,2)\n",
    "y_data1 = np.array([3,5,7,9,11]).reshape(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.random.rand(2,1)\n",
    "b1 = np.random.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.89591418]] [0.3489733]\n"
     ]
    }
   ],
   "source": [
    "w = np.random.rand(1,1) # y = Wx + b 를 y = xw + b 로 계산을 한다.\n",
    "b = np.random.rand(1)\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_func(x, t):\n",
    "#     y = np.dot(x, w) + b\n",
    "#     return (np.sum((t-y)**2)) / (len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(x, t):\n",
    "    y = np.dot(x, w1) + b1\n",
    "    return (np.sum((t-y)**2)) / (len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 편미분\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) #\n",
    "\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) #f(x + delta_x)\n",
    "\n",
    "        x[idx] = tmp_val - delta_x\n",
    "        fx2 = f(x) #f(x - delta_x)\n",
    "\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "\n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측함수\n",
    "def predict(x):\n",
    "  y=np.dot(x, w)+b\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict1(x):\n",
    "    y = np.dot(x, w1) + b1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value = 2.9149728434702625e-08 Initial w =  [[2.00011087]] b =  [0.99959982]\n",
      "step : 0 error value :  2.8949873171323095e-08 w= [[2.00011049]] b= [0.9996012]\n",
      "step : 100 error value :  1.454996322119555e-08 w= [[2.00007833]] b= [0.99971727]\n",
      "step : 200 error value :  7.312689367731682e-09 w= [[2.00005553]] b= [0.99979956]\n",
      "step : 300 error value :  3.675296286051773e-09 w= [[2.00003937]] b= [0.9998579]\n",
      "step : 400 error value :  1.8471730591451662e-09 w= [[2.00002791]] b= [0.99989926]\n",
      "step : 500 error value :  9.283736724567946e-10 w= [[2.00001979]] b= [0.99992858]\n",
      "step : 600 error value :  4.665928140518453e-10 w= [[2.00001403]] b= [0.99994937]\n",
      "step : 700 error value :  2.3450563128948516e-10 w= [[2.00000994]] b= [0.99996411]\n",
      "step : 800 error value :  1.1786056160581356e-10 w= [[2.00000705]] b= [0.99997455]\n",
      "step : 900 error value :  5.923572882541221e-11 w= [[2.000005]] b= [0.99998196]\n",
      "step : 1000 error value :  2.977138002312327e-11 w= [[2.00000354]] b= [0.99998721]\n",
      "step : 1100 error value :  1.4962845667578715e-11 w= [[2.00000251]] b= [0.99999093]\n",
      "step : 1200 error value :  7.520200618255236e-12 w= [[2.00000178]] b= [0.99999357]\n",
      "step : 1300 error value :  3.779589698085368e-12 w= [[2.00000126]] b= [0.99999544]\n",
      "step : 1400 error value :  1.8995900525423865e-12 w= [[2.0000009]] b= [0.99999677]\n",
      "step : 1500 error value :  9.547180128273859e-13 w= [[2.00000063]] b= [0.99999771]\n",
      "step : 1600 error value :  4.798332580451457e-13 w= [[2.00000045]] b= [0.99999838]\n",
      "step : 1700 error value :  2.411601673847633e-13 w= [[2.00000032]] b= [0.99999885]\n",
      "step : 1800 error value :  1.2120507536466084e-13 w= [[2.00000023]] b= [0.99999918]\n",
      "step : 1900 error value :  6.091665326487235e-14 w= [[2.00000016]] b= [0.99999942]\n"
     ]
    }
   ],
   "source": [
    "learning_rate=1e-2 # 1e - 2 : 1 * 10의 제곱을 의미 \n",
    "f = lambda x: loss_func(x_data, y_data)\n",
    "print(\"Initial error value =\", loss_func(x_data, y_data), \"Initial w = \", w, \"b = \", b)\n",
    "\n",
    "for step in range(2000):\n",
    "    w -= learning_rate * numerical_derivative(f, w)\n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print('step :', step, 'error value : ', loss_func(x_data, y_data), 'w=',w, 'b=', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value = 2.826918611731517 Initial w =  [[0.90189919]\n",
      " [0.65854489]] b =  [0.09791295]\n",
      "step : 0 error value :  0.4848113081341687 w= [[1.01321403]\n",
      " [0.80110392]] b= [0.11107353]\n",
      "step : 100 error value :  4.0178457529126334e-05 w= [[1.0957065 ]\n",
      " [0.90010861]] b= [0.11471583]\n",
      "step : 200 error value :  1.7777323342499086e-05 w= [[1.09827492]\n",
      " [0.89894139]] b= [0.11091947]\n",
      "step : 300 error value :  7.865738125825434e-06 w= [[1.09998337]\n",
      " [0.89816499]] b= [0.10839422]\n",
      "step : 400 error value :  3.4802672523922006e-06 w= [[1.10111979]\n",
      " [0.89764854]] b= [0.10671448]\n",
      "step : 500 error value :  1.5398758456366198e-06 w= [[1.10187571]\n",
      " [0.89730502]] b= [0.10559716]\n",
      "step : 600 error value :  6.813320495290869e-07 w= [[1.10237852]\n",
      " [0.89707651]] b= [0.10485395]\n",
      "step : 700 error value :  3.0146155161116684e-07 w= [[1.10271299]\n",
      " [0.89692452]] b= [0.10435958]\n",
      "step : 800 error value :  1.333844006937795e-07 w= [[1.10293547]\n",
      " [0.89682341]] b= [0.10403074]\n",
      "step : 900 error value :  5.9017139178467856e-08 w= [[1.10308345]\n",
      " [0.89675616]] b= [0.103812]\n",
      "step : 1000 error value :  2.6112669088011786e-08 w= [[1.10318189]\n",
      " [0.89671142]] b= [0.1036665]\n",
      "step : 1100 error value :  1.155378753350156e-08 w= [[1.10324737]\n",
      " [0.89668167]] b= [0.10356972]\n",
      "step : 1200 error value :  5.112078199246937e-09 w= [[1.10329092]\n",
      " [0.89666187]] b= [0.10350534]\n",
      "step : 1300 error value :  2.261885415412516e-09 w= [[1.10331989]\n",
      " [0.89664871]] b= [0.10346252]\n",
      "step : 1400 error value :  1.0007917393181884e-09 w= [[1.10333916]\n",
      " [0.89663995]] b= [0.10343404]\n",
      "step : 1500 error value :  4.428093919450785e-10 w= [[1.10335198]\n",
      " [0.89663413]] b= [0.10341509]\n",
      "step : 1600 error value :  1.959250360564176e-10 w= [[1.10336051]\n",
      " [0.89663025]] b= [0.10340249]\n",
      "step : 1700 error value :  8.668881115638182e-11 w= [[1.10336618]\n",
      " [0.89662767]] b= [0.1033941]\n",
      "step : 1800 error value :  3.835625159558132e-11 w= [[1.10336995]\n",
      " [0.89662596]] b= [0.10338853]\n",
      "step : 1900 error value :  1.6971071781697093e-11 w= [[1.10337246]\n",
      " [0.89662482]] b= [0.10338482]\n"
     ]
    }
   ],
   "source": [
    "# 값이 클때 발산을 할 수 있어서 손실 함수가 늘어난다.\n",
    "# 값이 작으면 데이터를 구할때 시간이 많이 소모된다.\n",
    "# 가설로 정한 그래프가 직선으로 내려오는게 아니고 구불 구불한 곡선이 되면 \n",
    "# 중간의 평평한 지점을 기울기 0으로 오류를 발생을 할 수 있다. 이렇게 되면 잘못된 결과 값이 나온다.\n",
    "learning_rate=1e-2 \n",
    "f = lambda x: loss_func(x_data1, y_data1)\n",
    "print(\"Initial error value =\", loss_func(x_data1, y_data1), \"Initial w = \", w1, \"b = \", b1)\n",
    "\n",
    "for step in range(2000):\n",
    "    w1 -= learning_rate * numerical_derivative(f, w1)\n",
    "    b1 -= learning_rate*numerical_derivative(f, b1)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print('step :', step, 'error value : ', loss_func(x_data1, y_data1), 'w=',w1, 'b=', b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15.00000039])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(np.array([7]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
